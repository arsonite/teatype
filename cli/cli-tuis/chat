#!/usr/bin/env python3.11

# Copyright (C) 2024-2025 Burak GÃ¼naydin
#
# Permission is hereby granted, free of charge, to any person obtaining a copy
# of this software and associated documentation files (the "Software"), to deal
# in the Software without restriction, including without limitation the rights
# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
# copies of the Software, and to permit persons to whom the Software is
# furnished to do so, subject to the following conditions:
#
# The above copyright notice and this permission notice shall be included in
# all copies or substantial portions of the Software.

# From package imports
from teatype import colorwrap
from teatype.ai.llm import ConversationalAI
from teatype.cli import BaseTUI
from teatype.io import fetch, path, prompt
from teatype.logging import *

DEFAULT_MODEL_PATH = ''
MANUAL_REFRESH = False # If True, the shell will be cleared automatically after each command
DEBUG_MODE = False

class Chat(BaseTUI):
    def meta(self):
        return {
            'name': 'chat',
            'shorthand': 'ch',
            'help': 'Chat with a local LLM model',
        }
        
    def on_display(self):
        pass
    
    def on_prompt(self):
        pass

    def run(self):
        println()
        
        parent_directory = path.caller_parent(reverse_depth=2)
        cli_dist_directory = path.join(parent_directory, 'dist')
        model_directory = path.join(cli_dist_directory, 'llm-models')
        conversational_model_directory = path.join(model_directory, 'conversational')
        if not path.exists(conversational_model_directory):
            warn(f'Conversational model directory not found at {conversational_model_directory}. Creating it. Please re-run this script after placing your model there.',
                 use_prefix=False)
            path.create(conversational_model_directory)
            println()
            return
        
        stream = self.get_flag('stream')
        if not stream:
            stream = True
            
        llm = ConversationalAI(model='Nous-Hermes-2-Mistral-7B-DPO.Q6_K',
                               model_directory=conversational_model_directory)
        
        try:
            while True:
                user_input = prompt('[You]:', return_bool=False)
                if user_input.lower() in ['exit', 'quit', 'q', 'bye']:
                    println()
                    break
                response = llm.chat(user_input, stream_response=stream)
                if not stream:
                    println(f'\n{colorwrap("Assistant: ", "green", bold=True)}{response}\n')
        except KeyboardInterrupt:
            println()
        
if __name__ == '__main__':
    Chat()